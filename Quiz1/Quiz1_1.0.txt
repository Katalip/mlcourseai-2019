1. Which of these problems does not fall into 3 main types of ML tasks: classification, regression, and clustering? *
1 балл



Identifying a topic of a live-chat with a customer



Grouping news into topics



Predicting LTV (Life-Time Value) - the amount of money spent by a customer in a certain large period of time



Listing top products that a user is prone to buy (based on his/her click history) answer

2. Maximal possible entropy is achieved when all states are equally probable (prove it yourself for a system with 2 states with probabilities p and (1-p)). What's the maximal possible entropy of a system with N states? (here all logs are with base 2) *
2 балла



N log N



- log N 



log N answer



- N log N
3. In Topic 3 article, toy example with 20 balls, what's the information gain of splitting 20 balls in 2 groups based on the condition X <= 8? *
2 балла



~ 0.1



~ 0.01 



~ 0.001



~ 0.0001 answer
4. ??? Which of these parameters have crucial effect on accuracy in a toy classification task (see notebook)? Select all that apply * 
2 балла




max_features answer




criterion




min_samples_leaf




max_depth answer
5. How many pure leaves are there in the tree? *
3 балла



6



7 answer



8



9

6. There are 7 jurors in the courtroom. Each of them individually can correctly determine whether the defendant is guilty or not with 80% probability. How likely is the jury will make a correct verdict jointly if the decision is made by majority voting? *
1 балл



20.97%



80.00%



83.70%



96.66% *

7. ??Which of the following is a better explanation of the visual difference between decision boundaries built by a single desicion tree and those built by ensemble models? *
1 балл



Ensembles ignore some of the features. Thus picking only important ones, they build a smoother decision boundary



Some of the classification rules built by a decision tree can be applied only to a small number of training instances *



When fitting a decision tree, if two potential splits are equally good in terms of information criterion, then a random split is chosen. This leads to some randomness in building a decision tree. Therefore its decision boundary is so jagged

8. Random Forest learns a coefficient for each input feature, which shows how much this feature influences the target feature. True/False? *
1 балл



True*



False
9. Suppose we fit RandomForestRegressor to predict age of a customer (a real task actually, good for targeting ads), and the maximal age seen in the dataset is 98 years. Is it possible that for some customer in future the model predicts his/her age to be 105 years? *
1 балл



Yes



No *
10. Select all statements supporting advantages of Random Forest over decision trees (some statements might be true but not about Random Forest's pros, don't select those). *
1 балл




Random Forest is easier to train in terms of computational resources




Random Forest typically requires more RAM than a single decision tree




Random Forest typically achieves better metrics in classification/regression tasks*




Single decision tree's prediction can be much easier interpreted